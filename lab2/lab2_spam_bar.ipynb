{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: Binary classification and model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Spam classification\n",
    "Each object in dataset is a letter with features based on text, spam is a positive example 1, normal letter - negative 0.\n",
    "\n",
    "dataset: Spambase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Spambase dataset downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...          0.00        0.000   \n",
       "1             0.00            0.94  ...          0.00        0.132   \n",
       "2             0.64            0.25  ...          0.01        0.143   \n",
       "3             0.31            0.63  ...          0.00        0.137   \n",
       "4             0.31            0.63  ...          0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are some problems with urllib2 or urllib.request...\n",
    "feature_names = [\n",
    "    line.strip().split(':')[0] \n",
    "    for line in open('spambase.names').readlines()[33:]\n",
    "]\n",
    "spam_data = pd.read_csv('spambase.data', header=None, names=(feature_names + ['spam']))\n",
    " \n",
    "X, y = spam_data.ix[:, :-1].values, spam_data.ix[:, -1].values\n",
    "spam_data.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows give information about frequensy of each word in each letter. th,e colums \"spam\" was added, 1- spam, 0 - regular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 How many letters in a dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4601 letters \n"
     ]
    }
   ],
   "source": [
    "print (\"There are\", len(spam_data), \"letters \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Which portion of them is bad (spam) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portion of spam is: 39.404477287546186 %\n"
     ]
    }
   ],
   "source": [
    "x1=len(spam_data.groupby('spam').get_group(1))\n",
    "x2=len(spam_data)\n",
    "x3=x1/x2*100\n",
    "print (\"portion of spam is:\", x3, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 How you can group letters' features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2788\n",
      "1    1813\n",
      "Name: spam, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('word_freq_', 48), ('char_freq_', 6), ('capital_run_', 3)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  by frequency of:\n",
    "# spam/regular\n",
    "#each word, numbers - as word, \n",
    "#symbol, - as char\n",
    "#length_average, length_longest,length_total  -capital_run_\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "print (spam_data['spam'].value_counts())\n",
    "\n",
    "y=[]\n",
    "for i in spam_data.columns.values:\n",
    "    result1 = re.findall(r'word_freq_', i)\n",
    "    result2 = re.findall(r'char_freq_', i)\n",
    "    result3 = re.findall(r'capital_run_', i)\n",
    "    if result1:\n",
    "        y=y+result1\n",
    "    elif result2:\n",
    "        y=y+result2\n",
    "    elif result3:\n",
    "        y=y+result3\n",
    "\n",
    "y2=collections.Counter(y)\n",
    "y2.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All letters were devided by 4 groups:\n",
    "1. Spam/regular group -  1813 spam, 2788 regular\n",
    "2. Type \"word\" - 48\n",
    "3. Type \"char\" - 6\n",
    "4. Type \"capital_run\" - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Classifier training and it's evaluation\n",
    "\n",
    "As a result of this task you should get following models and compare them between each other: \n",
    "Constant model / Decision tree / KNN /  KNN with rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Split dataset into to disjoint subsets: train - first 3000 examples, test - all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating data\n",
    "train = spam_data[0:3000]\n",
    "test = spam_data[3000:]\n",
    "\n",
    "train , y_train = train.drop('spam', 1), list(train['spam'])\n",
    "test, y_test = test.drop('spam', 1), list(test['spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Train decision tree with train. Classify examples from test. Calculate classification quality metrics: Accuracy, Precision, Recall, F1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.769519050593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    regular       1.00      0.77      0.87      1601\n",
      "       spam       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.77      0.87      1601\n",
      "\n",
      "Confusion_matrix:\n",
      "[[1232  369]\n",
      " [   0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ella\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "\n",
    "#settings\n",
    "model = DecisionTreeClassifier(criterion='gini',max_depth=7)\n",
    "model.fit(train, y_train)\n",
    "\n",
    "#prediction\n",
    "expected = y_test\n",
    "predicted = model.predict(test)\n",
    "target_names = ['regular','spam'] ##\n",
    "\n",
    "#quality\n",
    "print (\"accuracy=\",accuracy_score(expected, predicted))\n",
    "print (metrics.classification_report(expected, predicted,target_names=target_names))\n",
    "print( \"Confusion_matrix:\")\n",
    "print (metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality metrics:\n",
    "1. Accuracy = (TP + TN) / (TP + TN + FP + FN) — fraction of correct predictions\n",
    "2. Precision = TP / (TP + FP) — accuracy, fraction of real spam in the letters classified as spam\n",
    "3. Recall = TP / (TP + FN) — completeness, fraction of filtered spam\n",
    "4. F1 = 2TP / (2TP + FP + FN) — harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Which drawbacks does evaluation on this test have? How you can make evalution more informative?\n",
    "\n",
    "The first disadvantage is sorted dataset. There is a case when all letters are regular, wihout spam variants. So, testing isn't informative.\n",
    "\n",
    "Based on this, it is neccessary to make another test set. So, spamdata set should be shuffled before evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Repeat experiment with dataset shuffled before split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>22</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "535             0.00                0.0           0.00           0.0   \n",
       "415             0.62                0.0           0.62           0.0   \n",
       "3781            0.00                0.0           0.00           0.0   \n",
       "2541            0.00                0.0           0.00           0.0   \n",
       "3685            0.00                0.0           0.00           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "535            0.00             0.0              0.00                 0.0   \n",
       "415            1.25             0.0              0.62                 0.0   \n",
       "3781           0.00             0.0              0.00                 0.0   \n",
       "2541           0.00             0.0              0.00                 0.0   \n",
       "3685           0.00             0.0              0.00                 0.0   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "535               0.0             0.0  ...         0.000        0.421   \n",
       "415               0.0             0.0  ...         0.000        0.000   \n",
       "3781              0.0             0.0  ...         0.636        1.273   \n",
       "2541              0.0             0.0  ...         0.000        0.000   \n",
       "3685              0.0             0.0  ...         0.000        0.000   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "535           0.0        0.210        0.632          0.0   \n",
       "415           0.0        0.456        0.273          0.0   \n",
       "3781          0.0        0.000        0.000          0.0   \n",
       "2541          0.0        0.000        0.000          0.0   \n",
       "3685          0.0        0.000        0.000          0.0   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "535                         3.75                          15   \n",
       "415                         2.52                          22   \n",
       "3781                        3.50                          24   \n",
       "2541                        4.80                          19   \n",
       "3685                        1.00                           1   \n",
       "\n",
       "      capital_run_length_total  spam  \n",
       "535                         60     1  \n",
       "415                        121     1  \n",
       "3781                        35     0  \n",
       "2541                        24     0  \n",
       "3685                         4     0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffling data\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_shuf = spam_data\n",
    "data_shuf = shuffle(data_shuf)\n",
    "\n",
    "data_shuf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating data\n",
    "train = data_shuf[0:3000]\n",
    "test = data_shuf[3000:]\n",
    "\n",
    "train , y_train = train.drop('spam', 1), list(train['spam'])\n",
    "test, y_test = test.drop('spam', 1), list(test['spam'])\n",
    "\n",
    "target_names = ['regular','spam'] ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      "accuracy= 0.916302311056\n",
      "classification_report:              precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.92      0.95      0.93       983\n",
      "       spam       0.92      0.86      0.89       618\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1601\n",
      "\n",
      "Confusion_matrix:decision tree:\n",
      "[[934  49]\n",
      " [ 85 533]]\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "#settings\n",
    "model = DecisionTreeClassifier(criterion='gini',max_depth=7)\n",
    "model.fit(train, y_train)\n",
    "\n",
    "#prediction\n",
    "expected = y_test\n",
    "predicted1 = model.predict(test)\n",
    "predicted11 = model.predict_proba(test)\n",
    "\n",
    "#quality\n",
    "A1 = accuracy_score(expected, predicted1)\n",
    "R1 = metrics.classification_report(expected, predicted1,target_names=target_names)\n",
    "M1 = metrics.confusion_matrix(expected, predicted1)\n",
    "\n",
    "print( 'DECISION TREE')\n",
    "\n",
    "print (\"accuracy=\",A1)\n",
    "print (\"classification_report:\" , R1)\n",
    "print( \"Confusion_matrix:decision tree:\")\n",
    "print (M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Which features are the most informative? Use feature importance from DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 char_freq_$, importance = 0.42\n",
      "6 word_freq_remove, importance = 0.18\n",
      "51 char_freq_!, importance = 0.11\n",
      "24 word_freq_hp, importance = 0.08\n",
      "45 word_freq_edu, importance = 0.04\n",
      "56 capital_run_length_total, importance = 0.04\n",
      "15 word_freq_free, importance = 0.04\n",
      "23 word_freq_money, importance = 0.02\n",
      "55 capital_run_length_longest, importance = 0.01\n",
      "26 word_freq_george, importance = 0.01\n",
      "16 word_freq_business, importance = 0.01\n",
      "48 char_freq_;, importance = 0.01\n",
      "21 word_freq_font, importance = 0.01\n",
      "44 word_freq_re, importance = 0.01\n",
      "17 word_freq_email, importance = 0.00\n",
      "25 word_freq_hpl, importance = 0.00\n",
      "18 word_freq_you, importance = 0.00\n",
      "54 capital_run_length_average, importance = 0.00\n",
      "11 word_freq_will, importance = 0.00\n",
      "27 word_freq_650, importance = 0.00\n",
      "9 word_freq_mail, importance = 0.00\n",
      "47 word_freq_conference, importance = 0.00\n",
      "34 word_freq_85, importance = 0.00\n",
      "10 word_freq_receive, importance = 0.00\n",
      "50 char_freq_[, importance = 0.00\n",
      "1 word_freq_address, importance = 0.00\n",
      "2 word_freq_all, importance = 0.00\n",
      "4 word_freq_our, importance = 0.00\n",
      "3 word_freq_3d, importance = 0.00\n",
      "5 word_freq_over, importance = 0.00\n",
      "13 word_freq_report, importance = 0.00\n",
      "12 word_freq_people, importance = 0.00\n",
      "19 word_freq_credit, importance = 0.00\n",
      "8 word_freq_order, importance = 0.00\n",
      "7 word_freq_internet, importance = 0.00\n",
      "14 word_freq_addresses, importance = 0.00\n",
      "28 word_freq_lab, importance = 0.00\n",
      "20 word_freq_your, importance = 0.00\n",
      "22 word_freq_000, importance = 0.00\n",
      "53 char_freq_#, importance = 0.00\n",
      "49 char_freq_(, importance = 0.00\n",
      "46 word_freq_table, importance = 0.00\n",
      "43 word_freq_project, importance = 0.00\n",
      "42 word_freq_original, importance = 0.00\n",
      "41 word_freq_meeting, importance = 0.00\n",
      "40 word_freq_cs, importance = 0.00\n",
      "39 word_freq_direct, importance = 0.00\n",
      "38 word_freq_pm, importance = 0.00\n",
      "37 word_freq_parts, importance = 0.00\n",
      "36 word_freq_1999, importance = 0.00\n",
      "35 word_freq_technology, importance = 0.00\n",
      "33 word_freq_415, importance = 0.00\n",
      "32 word_freq_data, importance = 0.00\n",
      "31 word_freq_857, importance = 0.00\n",
      "30 word_freq_telnet, importance = 0.00\n",
      "29 word_freq_labs, importance = 0.00\n",
      "0 word_freq_make, importance = 0.00\n"
     ]
    }
   ],
   "source": [
    "most_important_features = np.argsort(model.feature_importances_)[::-1]\n",
    "for idx in most_important_features:\n",
    "    print ('%d %s, importance = %.2f' % (idx, spam_data.columns.values[idx], model.feature_importances_[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the most informative feature if frequency of char \"$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 What you can say about quality of the best constant model (constant model always predicts one class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy estimators\n",
    "\n",
    "classification:constant \n",
    "\n",
    "always predicts a constant label that is provided by the user.\n",
    "\n",
    "A major motivation of this method is F1-scoring, when the positive class is in the minority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSTANT MODEL C=0\n",
      "accuracy= 0.613991255465\n",
      "classification_report:              precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.61      1.00      0.76       983\n",
      "       spam       0.00      0.00      0.00       618\n",
      "\n",
      "avg / total       0.38      0.61      0.47      1601\n",
      "\n",
      "Confusion_matrix:decision tree:\n",
      "[[983   0]\n",
      " [618   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ella\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Constant=0 model\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "random_state = 0\n",
    "\n",
    "#settings\n",
    "model = DummyClassifier(strategy='constant', random_state=random_state, constant=0)\n",
    "model.fit(train, y_train)\n",
    "\n",
    "#prediction\n",
    "expected = y_test\n",
    "c0 = model.predict(test)\n",
    "\n",
    "#quality\n",
    "A2 = accuracy_score(expected, c0)\n",
    "R2 = metrics.classification_report(expected, c0,target_names=target_names)\n",
    "M2 = metrics.confusion_matrix(expected, c0)\n",
    "\n",
    "print (\"CONSTANT MODEL C=0\")\n",
    "\n",
    "print (\"accuracy=\",A2)\n",
    "print (\"classification_report:\" , R2)\n",
    "print( \"Confusion_matrix:decision tree:\")\n",
    "print (M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSTANT MODEL C=1\n",
      "accuracy= 0.386008744535\n",
      "classification_report:              precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.00      0.00      0.00       983\n",
      "       spam       0.39      1.00      0.56       618\n",
      "\n",
      "avg / total       0.15      0.39      0.22      1601\n",
      "\n",
      "Confusion_matrix:decision tree:\n",
      "[[  0 983]\n",
      " [  0 618]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ella\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Constant model\n",
    "\n",
    "#settings\n",
    "model = DummyClassifier(strategy='constant', random_state=random_state, constant=1)\n",
    "model.fit(train, y_train)\n",
    "\n",
    "#prediction\n",
    "expected = y_test\n",
    "c1 = model.predict(test)\n",
    "c11 = model.predict_proba(test)\n",
    "\n",
    "#quality\n",
    "A3 = accuracy_score(expected, c1)\n",
    "R3 = metrics.classification_report(expected, c1,target_names=target_names)\n",
    "M3 = metrics.confusion_matrix(expected, c1)\n",
    "\n",
    "print (\"CONSTANT MODEL C=1\")\n",
    "\n",
    "print (\"accuracy=\",A3)\n",
    "print (\"classification_report:\" , R3)\n",
    "print( \"Confusion_matrix:decision tree:\")\n",
    "print (M3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 Train KNN model and evaluate it on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "accuracy= 0.782635852592\n",
      "classification_report:              precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.80      0.85      0.83       983\n",
      "       spam       0.74      0.67      0.70       618\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1601\n",
      "\n",
      "Confusion_matrix:decision tree:\n",
      "[[840 143]\n",
      " [205 413]]\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#settings\n",
    "neigh = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "neigh.fit(train, y_train) \n",
    "\n",
    "#prediction\n",
    "expected = y_test\n",
    "predicted4=neigh.predict(test)\n",
    "predicted44 = neigh.predict_proba(test)\n",
    "\n",
    "#quality\n",
    "A4 = accuracy_score(expected, predicted4)\n",
    "R4 = metrics.classification_report(expected, predicted4,target_names=target_names)\n",
    "M4 = metrics.confusion_matrix(expected, predicted4)\n",
    "\n",
    "print (\"KNN\")\n",
    "\n",
    "print (\"accuracy=\",A4)\n",
    "print (\"classification_report:\" , R4)\n",
    "print( \"Confusion_matrix:decision tree:\")\n",
    "print (M4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Train KNN for rescaled features. Evaluate quality of the model on test. Does that feature transformation increased given metrics for KNN? Repeat experiment for DecisionTree. Why rescaling has no effect on the quality of decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>1.043072</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.072428</td>\n",
       "      <td>2.262263</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.045436</td>\n",
       "      <td>-0.190757</td>\n",
       "      <td>-0.368293</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.688194</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.673183</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.394588</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>1.292268</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.229197</td>\n",
       "      <td>0.802054</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.084206</td>\n",
       "      <td>-0.154835</td>\n",
       "      <td>-0.267680</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>2.454048</td>\n",
       "      <td>4.194823</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.053316</td>\n",
       "      <td>-0.144572</td>\n",
       "      <td>-0.409528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>-0.170230</td>\n",
       "      <td>-0.427671</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.460659</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2       3         4         5         6   \\\n",
       "0 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "1  1.688194 -0.165072  0.673183 -0.0469  1.394588 -0.350266  1.292268   \n",
       "2 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "3 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "4 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "\n",
       "         7         8         9  ...         48        49        50        51  \\\n",
       "0 -0.262562 -0.323302 -0.371364 ...  -0.158453  1.043072 -0.155198 -0.072428   \n",
       "1 -0.262562 -0.323302 -0.371364 ...  -0.158453 -0.514307 -0.155198  0.229197   \n",
       "2 -0.262562 -0.323302 -0.371364 ...   2.454048  4.194823 -0.155198 -0.329912   \n",
       "3 -0.262562 -0.323302 -0.371364 ...  -0.158453 -0.514307 -0.155198 -0.329912   \n",
       "4 -0.262562 -0.323302 -0.371364 ...  -0.158453 -0.514307 -0.155198 -0.329912   \n",
       "\n",
       "         52        53        54        55        56   57  \n",
       "0  2.262263 -0.103048 -0.045436 -0.190757 -0.368293  1.0  \n",
       "1  0.802054 -0.103048 -0.084206 -0.154835 -0.267680  1.0  \n",
       "2 -0.308355 -0.103048 -0.053316 -0.144572 -0.409528  0.0  \n",
       "3 -0.308355 -0.103048 -0.012341 -0.170230 -0.427671  0.0  \n",
       "4 -0.308355 -0.103048 -0.132116 -0.262599 -0.460659  0.0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rescaled features\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X = data_shuf.drop('spam', 1)\n",
    "X1= data_shuf['spam']\n",
    "\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "X_scaled1=np.column_stack((X_scaled, X1))\n",
    "type(X_scaled1)\n",
    "df = pd.DataFrame(X_scaled1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with rescaling\n",
      "accuracy= 0.904434728295\n",
      "classification_report:              precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.90      0.95      0.92       983\n",
      "       spam       0.92      0.83      0.87       618\n",
      "\n",
      "avg / total       0.91      0.90      0.90      1601\n",
      "\n",
      "Confusion_matrix:decision tree:\n",
      "[[936  47]\n",
      " [106 512]]\n"
     ]
    }
   ],
   "source": [
    "# KNN with rescaling\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#creating data\n",
    "train= df[0:3000]\n",
    "test = df[3000:]\n",
    "# 57 is \"spam\"\n",
    "train , y_train = train.drop(57, 1), list(train[57])\n",
    "test, y_test = test.drop(57, 1), list(test[57])\n",
    "\n",
    "#settings\n",
    "neigh = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "neigh.fit(train, y_train) \n",
    "\n",
    "#prediction\n",
    "expected = y_test\n",
    "predicted5=neigh.predict(test)\n",
    "predicted55 = neigh.predict_proba(test)\n",
    "\n",
    "#quality\n",
    "A5 = accuracy_score(expected, predicted5)\n",
    "R5 = metrics.classification_report(expected, predicted5,target_names=target_names)\n",
    "M5 = metrics.confusion_matrix(expected, predicted5)\n",
    "\n",
    "print (\"KNN with rescaling\")\n",
    "\n",
    "print (\"accuracy=\",A5)\n",
    "print (\"classification_report:\" , R5)\n",
    "print( \"Confusion_matrix:decision tree:\")\n",
    "print (M5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does rescaled feature increased metrics for KNN?\n",
    "\n",
    " -Yes, transformation makes results better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rescaling for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with rescaling\n",
      "accuracy= 0.916926920675\n",
      "classification_report:              precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.92      0.95      0.93       983\n",
      "       spam       0.92      0.86      0.89       618\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1601\n",
      "\n",
      "Confusion_matrix:decision tree:\n",
      "[[934  49]\n",
      " [ 84 534]]\n"
     ]
    }
   ],
   "source": [
    "# Desicion Tree with rescaling\n",
    "\n",
    "#settings\n",
    "model = DecisionTreeClassifier(criterion='gini',max_depth=7)\n",
    "model.fit(train, y_train)\n",
    "\n",
    "#prediction\n",
    "expected = y_test\n",
    "predicted = model.predict(test)\n",
    "\n",
    "#quality\n",
    "A6 = accuracy_score(expected, predicted)\n",
    "R6 = metrics.classification_report(expected, predicted,target_names=target_names)\n",
    "M6 = metrics.confusion_matrix(expected, predicted)\n",
    "\n",
    "print (\"Decision Tree with rescaling\")\n",
    "\n",
    "print (\"accuracy=\",A6)\n",
    "print (\"classification_report:\" , R6)\n",
    "print( \"Confusion_matrix:decision tree:\")\n",
    "print (M6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same results as without rescaling decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 Compare all models by all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "  constant model,c=1 0.386008744535\n",
      "  deciion tree 0.916302311056\n",
      "  KNN 0.782635852592\n",
      "  KNN with rescaling 0.904434728295\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy')\n",
    "print ('  constant model,c=1', A3)\n",
    "print ('  deciion tree', A1)\n",
    "print ('  KNN', A4)\n",
    "print ('  KNN with rescaling', A5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification_report\n",
      "\n",
      "constant model,c=1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.00      0.00      0.00       983\n",
      "       spam       0.39      1.00      0.56       618\n",
      "\n",
      "avg / total       0.15      0.39      0.22      1601\n",
      "\n",
      "----------------------------------------------------------\n",
      "decision tree\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.92      0.95      0.93       983\n",
      "       spam       0.92      0.86      0.89       618\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1601\n",
      "\n",
      "----------------------------------------------------------\n",
      "KNN\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.80      0.85      0.83       983\n",
      "       spam       0.74      0.67      0.70       618\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1601\n",
      "\n",
      "----------------------------------------------------------\n",
      "KNN with rescaling\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    regular       0.90      0.95      0.92       983\n",
      "       spam       0.92      0.83      0.87       618\n",
      "\n",
      "avg / total       0.91      0.90      0.90      1601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Classification_report\")\n",
    "print(\"\")\n",
    "print ('constant model,c=1')\n",
    "print (R3)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print ('decision tree')\n",
    "print (R1)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print ('KNN')\n",
    "print(R4)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print ('KNN with rescaling')\n",
    "print(R5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ella\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:161: UserWarning: pylab import has clobbered these variables: ['test', 'shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Which classifier has AUC-ROC near 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "AUC-ROC - Area Under the Receiver Operating Characteristic curve. \n",
    "\n",
    "Interpritation of AUC-ROC:\n",
    "The expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\n",
    "\n",
    "A value of less than 0.5 says that the classifier operates exactly the opposite: if the positive are called negative and vice versa, the classifier will work better. \n",
    "so, it means that in 0 everising is inverse. replacing true anf false variables will give the value 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Is it correct, that classifier from previous part #2 to identify class compare some estimation with threshold? What are those estimations? What thresholds were chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree thresholds::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12735415 -0.15127225  0.14704713  0.05589417 -0.19039997  0.0134531\n",
      " -0.31983888 -2.         -2.          0.49261808 -2.         -2.\n",
      "  0.03338979  0.07210261 -2.         -2.         -2.         -0.17146352\n",
      " -0.08762041  0.97632313 -2.         -2.         -0.11900382 -2.         -2.\n",
      " -2.         -0.3790139   0.88549131  1.16532743  0.90172201 -2.         -2.\n",
      " -2.          1.51630378 -2.         -2.         -0.12603889  2.29606199\n",
      " -0.19417827 -2.         -2.         -2.         -2.         -0.19118634\n",
      " -0.07115535 -0.12691249  3.47547364  7.11752844 -2.         -2.         -2.\n",
      " -2.         -2.          0.64560735 -2.         -2.         -0.08946107\n",
      "  0.02214666 -0.2292439   0.56411064  1.26340103 -2.          0.24074855\n",
      " -2.         -2.         -2.         -0.01734618  0.07520369  1.89944577\n",
      " -2.         -2.          0.58525985 -2.         -2.          0.47257259\n",
      " -2.          1.60762107 -2.         -2.          0.47840142  0.60968769\n",
      " -2.         -2.         -2.          0.16995177  0.76783442 -2.\n",
      "  0.04352772 -2.         -2.         -2.        ]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(criterion='gini',max_depth=7)\n",
    "model.fit(train, y_train)\n",
    "print (model.tree_.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In decision tree there was used criterion='gini'. Criterion is the function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Draw ROC and Precision/Recall curves on the same coordinate plane with different colors. Add legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (1601, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4b23907634ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfpr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ella\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \"\"\"\n\u001b[1;32m    504\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 505\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ella\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ella\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (1601, 2)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#train = data_shuf[0:3000] ##\n",
    "#test = data_shuf[3000:]   ##\n",
    "#train , y_train = train.drop('spam', 1), list(train['spam']) ##\n",
    "#test, y_test = test.drop('spam', 1), list(test['spam'])      ##\n",
    "\n",
    "# Desicion Tree with rescaling\n",
    "model = DecisionTreeClassifier(criterion='gini',max_depth=7)\n",
    "model.fit(train, y_train)\n",
    "exp1 = y_test\n",
    "pred1 = model.predict_proba(test)\n",
    "\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(exp1, pred1)\n",
    "roc_auc = metrics.auc(fpr1, tpr1)\n",
    "\n",
    "#KNN\n",
    "neigh = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "neigh.fit(train, y_train) \n",
    "exp2 = y_test\n",
    "pred2=neigh.predict_proba(test)\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(exp2, pred2)\n",
    "\n",
    "#constant=1 model\n",
    "model = DummyClassifier(strategy='constant', random_state=random_state, constant=1)\n",
    "model.fit(train, y_train)\n",
    "exp3 = y_test\n",
    "c1 = model.predict_proba(test)\n",
    "fpr3, tpr3, thresholds3 = metrics.roc_curve(exp3, c1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('AUC-ROC')\n",
    "\n",
    "plt.plot(fpr1, tpr1, label = 'DecTree = %.2f' % roc_auc_score(np.array(y_test), np.array(pred1)))\n",
    "plt.plot(fpr2, tpr2, label = 'KNN = %.2f' % roc_auc_score(np.array(y_test), np.array(pred2)))\n",
    "plt.plot(fpr3, tpr3, label = 'constant_1 = %.2f' % roc_auc_score(np.array(y_test), np.array(c1)))\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision/Recall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Precision/Recall plot\n",
    "plt.figure()\n",
    " \n",
    "recr1,precr1,thresholds1 = metrics.precision_recall_curve(exp1, pred1)\n",
    "recr2,precr2,thresholds2 = metrics.precision_recall_curve(exp2, pred2)\n",
    "recr3,precr3, thresholds3 = metrics.precision_recall_curve(exp3, c1)\n",
    "\n",
    "\n",
    "plt.plot( recr1,precr1, label = 'DecTree')\n",
    "plt.plot( recr2,precr2, label = 'KNN')\n",
    "plt.plot( recr3,precr3, label = 'constant model class 1')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Recall Rate')\n",
    "plt.ylabel('Precision Rate')\n",
    "plt.title('Precision/Recall plot')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Compare AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bar chart\n",
    "auc1 = roc_auc_score(np.array(y_test), np.array(pred1))\n",
    "auc2 = roc_auc_score(np.array(y_test), np.array(pred2))\n",
    "auc3 = roc_auc_score(np.array(y_test), np.array(c1))\n",
    "\n",
    "objects = ('Decision tree', 'KNN', 'Constant')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [auc1,auc2,auc3]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('mean(AUC-ROC)')\n",
    "plt.title('model')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-validation and parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search : pick several values for every parameter, then evaluate every combination of parameters, and choose the best set from the point of optimized metric.grid search : pick several values for every parameter, then evaluate every combination of parameters, and choose the best set from the point of optimized metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Choose cross-validation method. Fixate cross-validation split of train set. You should cross-validate on train samples from previous tasks, test should stay independent. Pay attention: when comparing models, cross-validation split must not change. #### 4.3 Find optimal set of parameters for tree with grid search. Parameters for grid: split criterion, max depth, number of features for node, min number of objects in the leaf (of only some of offered parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#for decision tree classifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Parameters for grid: split criterion, max depth, number of features for node, min number of objects in the leaf (of only some of offered parameters)\n",
    "parameters = {'criterion':('gini', 'entropy'),'max_depth':[1, 2, 3, 4], 'max_features': (\"auto\", \"sqrt\", ),'min_samples_leaf': [1, 2, 3, 4]}\n",
    "\n",
    "model = GridSearchCV(DecisionTreeClassifier(), parameters) \n",
    "model.fit(train, y_train)\n",
    "print (sorted(model.cv_results_.keys()))\n",
    "print('')\n",
    "print (\"best_estimator:\", model.best_estimator_)\n",
    "print (\"best_parameters:\", model.best_params_)\n",
    "print (\"best_score:\", model.best_score_)\n",
    "print(\"scorer=\",model.scorer_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4.2 Choose one metric for optimization by grid search. Example: AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print('------------ AUC-ROC -------------------------')\n",
    "\n",
    "model1 = GridSearchCV(DecisionTreeClassifier(), parameters, scoring='roc_auc', n_jobs=4, cv=None)\n",
    "model1.fit(train, y_train)\n",
    "print (sorted(model1.cv_results_.keys()))\n",
    "print('')\n",
    "print (\"AUC-ROC.best_estimator:\", model1.best_estimator_)\n",
    "print (\"AUC-ROC.best_parameters:\", model1.best_params_)\n",
    "print (\"AUC-ROC.best_score:\", model1.best_score_)\n",
    "print(\"AUC-ROC.scorer=\",model1.scorer_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Find optimal set of parameters for KNN using grid search. Parameters for grid: K, metric, weight scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print ('for knn')\n",
    "parameters = {'n_neighbors':[7, 5, 2,3],'metric': ('euclidean','chebyshev','minkowski'),'weights':('uniform','distance')}\n",
    "\n",
    "model = GridSearchCV(KNeighborsClassifier(), parameters, scoring='roc_auc', n_jobs=6, cv=5)\n",
    "model.fit(train, y_train)\n",
    "\n",
    "#print (sorted(model.cv_results_.keys()))\n",
    "#print (\"best_estimator:\", model.best_estimator_)\n",
    "#print(\"scorer=\",model.scorer_)\n",
    "#print (\"best_score:\", model.best_score_)\n",
    "print (\"optimal set of parameters:\", model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09.04.17\n",
    "#### precision - recall curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
